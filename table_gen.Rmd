---
title: "DWCP_Simulation_Design_CPS"
output: html_notebook
---

To begin we will import the necessary libraries to replicate the code from Python to R. Ensure all packages are installed before attempting to run the file.
```{r}
pck <- c("Matrix", "CVXR", "data.table", "ggplot2", "parallel", "synthdid", "caret", "MASS", "MCPanel", "npmr")
lapply(pck, library, character.only = TRUE) 
```


We begin by replicating the decomposition functions. The order of the functions is changed for accessibility.
```{r}
decompose_Y <- function(Y, rank = 4) {
  N <- nrow(Y)
  T_ <- ncol(Y)
  
  # Perform Singular Value Decomposition
  svd_result <- svd(Y)
  u <- svd_result$u
  s <- svd_result$d
  v <- svd_result$v
  
  # Extract the rank components
  factor_unit <- u[, 1:rank]
  factor_time <- t(v)[1:rank, ]
  
  # Calculate the low-rank approximation matrix L
  L <- factor_unit %*% diag(s[1:rank]) %*% factor_time
  
  # Calculate the residual matrix E
  E <- Y - L
  
  # Calculate F and M matrices
  F_ <- outer(rowMeans(L), colMeans(L), `+`) - mean(L)
  M <- L - F_
  
  # Return the results
  list(F_ = F_, M = M, E = E, factor_unit_scaled = factor_unit * sqrt(N))
}
```
Now we include the helper functions needed for run_MCNNM
```{r}
getPO <- function(A, O) {
  # Create an output matrix initialized to zeros with the same dimensions as A
  A_out <- matrix(0, nrow = nrow(A), ncol = ncol(A))
  
  # Use the indices in O to copy elements from A to A_out
  A_out[cbind(O[,1], O[,2])] <- A[cbind(O[,1], O[,2])]
  
  return(A_out)
}

getPOinv <- function(A, O) {
  # Create a copy of A to modify
  A_out <- A
  
  # Set elements at specified indices in O to 0
  A_out[cbind(O[,1], O[,2])] <- 0
  
  return(A_out)
}

shrink_lambda <- function(A, lambd) {
  # Perform Singular Value Decomposition
  svd_result <- svd(A)
  S <- svd_result$u
  Σ <- svd_result$d
  R <- t(svd_result$v)
  
  # Shrink the singular values by lambda
  Σ <- Σ - lambd
  Σ[Σ < 0] <- 0
  
  # Reconstruct the matrix with the modified singular values
  S %*% diag(Σ) %*% R
}

```
With the helper functions now defined we can write out the function run_MCNNM
```{r}
run_MCNNM <- function(Y_obs, O, lambd = 10, threshold = 0.01, print_every = NULL, max_iters = 20000) {
  
  # Initialize L_prev with values from Y_obs outside the observed entries
  L_prev <- getPOinv(Y_obs, O)
  change <- 1000
  iters <- 0
  
  while ((change > threshold) && (iters < max_iters)) {
    
    # Get observed and unobserved parts
    PO <- getPO(Y_obs, O)
    PO_inv <- getPOinv(L_prev, O)
    
    # Update L_star and shrink the matrix
    L_star <- PO + PO_inv
    L_new <- shrink_lambda(L_star, lambd)
    
    # Calculate the change
    change <- norm(L_prev - L_new, type = "F")
    
    # Update L_prev and iteration count
    L_prev <- L_new
    iters <- iters + 1
    
    # Print progress if requested
    if (!is.null(print_every) && (iters %% print_every == 0)) {
      cat("Iteration:", iters, "Change:", change, "\n")
    }
  }
  
  return(L_new)
}
```

```{r}
DIFP_TWFE <- function(Y, W, treated_units, treated_periods) {
  
  # Define indices for treated and control periods
  T_post <- seq((ncol(Y) - treated_periods + 1), ncol(Y))  # Treated periods (last 10 columns)
  T_pre <- seq_len(ncol(Y) - treated_periods)              # Pre-treatment periods
  
  # Remove treated units and periods for control group matrix (X)
  X <- t(Y[-treated_units, T_pre])  # Control group: pre-treatment data only
  
  # Mean of treated units in the pre-treatment period
  y <- colMeans(Y[treated_units, T_pre, drop = FALSE])
  
  # Number of control units
  control_units <- ncol(X)
  
  # CVXR variables for optimization
  unit_weights <- CVXR::Variable(control_units, nonneg = TRUE)
  intercept <- CVXR::Variable()
  
  # Constraints for optimization
  constraints <- list(sum(unit_weights) == 1)  # Weights sum to 1
  
  # Objective function: minimize squared differences between treated and control
  objective <- sum_squares(y - X %*% unit_weights - intercept)
  
  # Solve the optimization problem
  prob <- CVXR::Problem(CVXR::Minimize(objective), constraints)
  result <- CVXR::solve(prob, solver = "ECOS")
  
  # Extract weights and intercept
  estimated_weights <- result$getValue(unit_weights)
  estimated_intercept <- result$getValue(intercept)
  
  # Predict counterfactual outcomes for treated units in treated periods
  X_predict <- t(Y[-treated_units, T_post])  # Control group data in post-treatment period
  y_predict <- X_predict %*% estimated_weights + estimated_intercept  # Counterfactual predictions
  
  # Observed outcomes of treated units in treated periods
  y_actual <- colMeans(Y[treated_units, T_post, drop = FALSE])
  
  # Compute treatment effect
  treatment_effect <- mean(y_actual - y_predict)
  
  # Return treatment effect
  return(treatment_effect)
}


```



Below is the cell corresponding to the DWCP_TWFE_average 
```{r}
DWCP_TWFE_average <- function(Y, W, treated_units, lambda_unit, lambda_time, lambda_nn, treated_periods = 10) {
  
  N <- nrow(Y)
  T_ <- ncol(Y)
  
  # dist_time
  dist_time <- abs(1:T_ - (T_ - treated_periods / 2))
  
  # dist_unit
  average_treated <- rowMeans(Y[treated_units, , drop = FALSE])
  mask <- matrix(1, N, T_)
  mask[, (T_ - treated_periods + 1):T_] <- 0
  A <- rowSums((Y - matrix(average_treated, nrow = N, ncol = T_, byrow = TRUE))^2 * mask)
  B <- rowSums(mask)
  dist_unit <- sqrt(A / B)
  
  # distance-based weights
  delta_unit <- exp(-lambda_unit * dist_unit)
  delta_time <- exp(-lambda_time * dist_time)
  delta <- outer(delta_unit, delta_time)
  
  # CVXR variables
  unit_effects <- Variable(1, N)
  time_effects <- Variable(1, T_)
  mu <- Variable()
  tau <- Variable()
  L <- Variable(N, T_)
  
  unit_factor <- t(kronecker(matrix(1, T_, 1), unit_effects))
  time_factor <- kronecker(matrix(1, N, 1), time_effects)
  
  # Objective function
  if (lambda_nn == Inf) {
    objective <- sum_squares((Y - mu - unit_factor - time_factor - L - W * tau) * delta)
  } else {
    objective <- sum_squares((Y - mu - unit_factor - time_factor - L - W * tau) * delta) + lambda_nn * cvxr_norm(L, "nuc")
  }
  
  # Constraints
  constraints <- list()
  
  # Problem setup and solve
  prob <- Problem(Minimize(objective), constraints)
  result <- solve(prob)
  
  return(result$getValue(tau))
}

```
Below we will implement the cross validation procedure
```{r}
get_CV_score <- function(Y_obs, O, lambd, n_folds = 4, verbose = FALSE) {
  
  # Initialize K-fold cross-validation
  folds <- createFolds(1:nrow(O), k = n_folds, list = TRUE)
  
  mse <- 0
  for (i in seq_along(folds)) {
    Otr_idx <- folds[[i]]
    Otst_idx <- setdiff(1:nrow(O), Otr_idx)
    
    Otr <- O[Otr_idx, , drop = FALSE]
    Otst <- O[Otst_idx, , drop = FALSE]
    
    if (verbose) cat(".")

    L <- run_MCNNM(Y_obs, Otr, lambd, threshold = 1e-10, print_every = NULL, max_iters = 20000)
    
    # Calculate mean squared error for the test set
    mse <- mse + sum((Y_obs[Otst] - L[Otst]) ^ 2)
  }
  
  # Return the average MSE over all folds
  return(mse / n_folds)
}

```
Now that we have the helper function we can implement the function for performing the CV
```{r}
do_CV <- function(Y_obs, O, lambdas = c(5, 10, 20, 40), n_tries = 10, verbose = FALSE) {
  
  score <- list()
  
  for (t in seq_len(n_tries)) {
    run_score <- list()
    for (l in lambdas) {
      if (verbose) cat(sprintf("lambda %d:", l))
      run_score[[as.character(l)]] <- get_CV_score(Y_obs, O, l, n_folds = 4, verbose = verbose)
      if (verbose) cat(sprintf(" : %f\n", run_score[[as.character(l)]]))
    }
    score[[as.character(t)]] <- run_score
  }
  
  return(score)
}

```


For smoothness of code we define helper functions to calculate the unit and time based weights
```{r}
# Define distance functions
dist_time <- function(s, T, T_treat) {
  abs(s - (T - T_treat / 2))
}

dist_unit <- function(j, Y, W, N_treat, T, T_treat) {
  
  N <- nrow(Y)
  
  untreated_time <- 1:(T - T_treat)
  Y_j_untreated <- mean(Y[j, untreated_time])
  Y_treated_avg <- mean(Y[(N - N_treat + 1):N, untreated_time])

  squared_diffs <- sum((Y[j, untreated_time] - Y_treated_avg)^2)
  sqrt(squared_diffs / (T - T_treat))
}

# Define weight calculation
calculate_weights <- function(lambda_time, lambda_unit, Y, W, N_treat, T, T_treat) {
  
  
  
  weights <- matrix(0, nrow = nrow(Y), ncol = ncol(Y))
  for (j in 1:nrow(Y)) {
    for (s in 1:ncol(Y)) {
      if (W[j, s] == 0) {
        time_dist <- dist_time(s, T, T_treat)
        unit_dist <- dist_unit(j, Y, W, N_treat, T, T_treat)
        weights[j, s] <- exp(-lambda_time * time_dist - lambda_unit * unit_dist)
      }
    }
  }
  weights
}
```

Objective Function
```{r}
# Objective function
objective <- function(params, Y, W, weights, placebo_units, lambda_nn) {
  tau <- params[1]
  mu <- params[2]
  alpha <- params[3]
  beta <- params[4]
  L <- matrix(params[-(1:4)], nrow = nrow(Y), ncol = ncol(Y))
  
  T_treat <- ncol(W) - 9

  W_placebo <- W
  W_placebo[placebo_units, (ncol(W) - T_treat + 1):ncol(W)] <- 1
  
  residuals <- weights * ((Y - (mu + alpha + beta + L - W_placebo * tau))^2)
  penalty <- lambda_nn * nuclear(L)
  sum(residuals) + penalty
}
```

The new cross validation procedure
```{r}
# Cross-validation function
cross_validation <- function(Y, W, lambda_grid, num_runs = 500) {
  N <- nrow(Y)
  T_ <- ncol(Y)
  
  N_treat <- nrow(W) - 9
  T_treat <- ncol(W) - 9
  
  best_lambda <- NULL
  best_std_dev <- Inf

  for (lambda_time in lambda_grid) {
    for (lambda_unit in lambda_grid) {
      for (lambda_nn in lambda_grid) {
        weights <- calculate_weights(lambda_time, lambda_unit, Y, W, N_treat, T_, T_treat)
        aate_estimates <- numeric(num_runs)
        
        for (run in 1:num_runs) {
          control_units <- which(rowSums(W) == 0)
          placebo_units <- sample(control_units, N_treat, replace = FALSE)

          init_params <- c(0, 0, 0, 0, rep(0, length(Y)))
          
          print(class(init_params))
          print(class(Y))
          print(class(W))
          print(class(weights))
          print(class(placebo_units))
          
          
          
          result <- optim(init_params, fn = objective, Y = Y, W = W, weights = weights,
                          placebo_units = placebo_units, lambda_nn = lambda_nn)

          if (result$convergence == 0) {
            ate_estimates[run] <- result$value
          }
        }

        std_dev <- sd(ate_estimates)
        if (std_dev < best_std_dev) {
          best_std_dev <- std_dev
          best_lambda <- c(lambda_time, lambda_unit, lambda_nn)
        }
      }
    }
  }
  list(best_lambda = best_lambda, best_std_dev = best_std_dev)
}

```


```{r}
generate_data <- function(F_, M, cov_mat, pi, noise = "norm", treated_periods = 10, treated_units = 10) {
  
  N <- nrow(F_)
  T_total <- ncol(F_)
  
  # Y = F + M + multivariate normal noise
  if (noise == "norm"){
    Y <- F_ + M + MASS::mvrnorm(n = N, mu = rep(0, T_total), Sigma = cov_mat)
  }else if (noise == "none"){
    Y <- F_ + M
  }else if (noise == "noise"){
    Y <- MASS::mvrnorm(n = N, mu = rep(0, T_total), Sigma = cov_mat)
  }
  
  
  # Initialize W matrix
  W <- matrix(0, nrow = N, ncol = T_total)
  
  # Generate treatment candidates
  candidates <- rbinom(N, 1, pi)
  
  treated_number <- sum(candidates)
  
  if (treated_number == 0) {
    index <- sample(1:N, 1)
  } else {
    index <- which(candidates == 1)
    if (treated_number > treated_units) {
      index <- sample(index, treated_units, replace = FALSE)
    }
  }
  
  # Set treatment in last treated_periods columns for treated units
  W[index, (T_total - treated_periods + 1):T_total] <- 1
  
  return(list(Y = Y, W = W, index = index))
}

```

AR functions
```{r}
fit_ar2 <- function(E) {
  
  T_full <- ncol(E)
  E_ts <- E[, 3:T_full]
  E_lag_1 <- E[, 2:(T_full - 1)]
  E_lag_2 <- E[, 1:(T_full - 2)]
  
  a_1 <- sum(diag(crossprod(E_lag_1, E_lag_1)))
  a_2 <- sum(diag(crossprod(E_lag_2, E_lag_2)))
  a_3 <- sum(diag(crossprod(E_lag_1, E_lag_2)))
  
  matrix_factor <- matrix(c(a_1, a_3, a_3, a_2), nrow = 2, byrow = TRUE)
  
  b_1 <- sum(diag(crossprod(E_lag_1, E_ts)))
  b_2 <- sum(diag(crossprod(E_lag_2, E_ts)))
  
  ar_coef <- solve(matrix_factor) %*% c(b_1, b_2)
  
  return(ar_coef)
}


ar2_correlation_matrix <- function(ar_coef, T_) {
  
  result <- numeric(T_)
  result[1] <- 1
  result[2] <- ar_coef[1] / (1 - ar_coef[2])
  
  for (t in 3:T_) {
    result[t] <- ar_coef[1] * result[t - 1] + ar_coef[2] * result[t - 2]
  }
  
  index_matrix <- abs(outer(1:T_, 1:T_, "-"))
  cor_matrix <- matrix(result[index_matrix + 1], nrow = T_, ncol = T_)
  
  return(cor_matrix)
}

```


Below we begin the process of running the baseline simulation and recreate the first row of Table 9 of the paper.

Preprocessing the data
```{r}
df <- read.csv('data/CPS.csv', sep = ';')
Y_true_full <- matrix(df$log_wage, nrow = 40, byrow = TRUE)
Y_true_full <- t(Y_true_full)
Y_true_full <- Y_true_full / sd(Y_true_full)
Y_true_full <- Y_true_full - mean(Y_true_full)
N_total <- nrow(Y_true_full)
T_total <- ncol(Y_true_full)
W_true_full <- matrix(0, N_total, T_total)

#creating the assignment vector for minimum wage
min_wage <- matrix(df$min_wage, nrow = 40, byrow = TRUE)
min_wage <- t(min_wage)
Ds <- which(min_wage == TRUE, arr.ind = TRUE)[, 1]
assignment_vector <- numeric(N_total)
assignment_vector[Ds] <- 1

#creating the assignment vector for gun laws
open_carry <- matrix(df$open_carry, nrow = 40, byrow = TRUE)
open_carry <- t(open_carry)
Ds_guns <- which(open_carry == TRUE, arr.ind = TRUE)[, 1]
assignment_vector_guns <- numeric(N_total)
assignment_vector_guns[Ds_guns] <- 1

#creating the assignment vector for gun laws
abort_ban <- matrix(df$abort_ban, nrow = 40, byrow = TRUE)
abort_ban <- t(abort_ban)
Ds_abort <- which(abort_ban == TRUE, arr.ind = TRUE)[, 1]
assignment_vector_abort <- numeric(N_total)
assignment_vector_abort[Ds_abort] <- 1
```



```{r}
print(assignment_vector_abort)
print(assignment_vector_guns)
```




Begin with the new cross validation method to find the best lambda triplets to use
```{r}
valid <- cross_validation(Y_true_full, W_true_full, 10^seq(-4, 2, length.out = 10)
, num_runs = 500)
lambda_unit <- valid$best_lambda[1]
lambda_time <- valid$best_lambda[2]
lambda_nn <- valid$best_lambda[3]
```

Generate parameters for data generation
```{r}
result <- decompose_Y(Y_true_full, rank = 4)

F_ <- result$F_
M <- result$M
E <- result$E
unit_factors <- result$factor_unit_scaled


ar_coef <- fit_ar2(E)
cor_matrix <- ar2_correlation_matrix(ar_coef, T_total)

# Calculate scaled_sd
scaled_sd <- norm(crossprod(t(E)) / N_total, type = "F") / norm(cor_matrix, type = "F")

# Calculate cov_mat
cov_mat <- cor_matrix * scaled_sd

# Fit logistic regression model
#Creating probabilities for each assignment models 
model <- glm(assignment_vector ~ unit_factors - 1, family = binomial)
pi <- predict(model, newdata = as.data.frame(unit_factors), type = "response")

model_g <- glm(assignment_vector_guns ~ unit_factors - 1, family = binomial)
pi_g <- predict(model_g, newdata = as.data.frame(unit_factors), type = "response")

model_a <- glm(assignment_vector_abort ~ unit_factors - 1, family = binomial)
pi_a <- predict(model_a, newdata = as.data.frame(unit_factors), type = "response")

# Calculate conditional variance
cond_var <- cov_mat[-1, -1] - (cov_mat[-1, (ncol(cov_mat)-2):(ncol(cov_mat)-1)] %*% 
              solve(cov_mat[(ncol(cov_mat)-2):(ncol(cov_mat)-1), (ncol(cov_mat)-2):(ncol(cov_mat)-1)]) %*% 
              cov_mat[(ncol(cov_mat)-2):(ncol(cov_mat)-1), -1])

```

```{r}
DIDp <- function( M, mask ){
  M <- M * mask
  treated_rows <- which(rowMeans(mask) < 1)
  control_rows <- setdiff(1:nrow(M), treated_rows)
  num_treated <- length(treated_rows)
  num_control <- length(control_rows)
  M_control_rows <- M[control_rows,]
  M_pred <- M
  for (l in 1:num_treated){
    tr_row_pred <- treated_rows[l]
    tr_row_miss_cols <- which(mask[treated_rows[l],]==0)
    control_cols <- setdiff(1:ncol(M),tr_row_miss_cols)
    W = (1/num_control) * rep(1L, num_control)
    mu = mean(M[tr_row_pred, control_cols]) - mean(M[control_rows, control_cols])
    M_pred_this_row = t(M_control_rows) %*% W+mu;
    M_pred[treated_rows[l],]=M_pred_this_row
  }
  return(M_pred)
}
```

Running the baseline simulation
```{r}

table_generation <- function(fixed_effects, interactive_data, cov_mat, prob, noise = "norm", treatment_periods, treatment_units, exp_num = 1000, ran_seed=0){
  
  #Setting random seed for reproducibility
  set.seed(ran_seed)
  
  
  #Initializing the vectors for each of the estimators
  estimate_sdid = rep(0,exp_num)
  estimate_dwcp = rep(0,exp_num)
  estimate_mc = rep(0,exp_num)
  estimate_sc = rep(0,exp_num)
  estimate_difp = rep(0,exp_num)
  estimate_did = rep(0,exp_num)
  
  #Running the experiment exp_num times for robustness default is 1000 as used in SDID paper
  for (expirement in 1:exp_num){
    
    #Generating data based on simulation requirements
    data <-generate_data(fixed_effects, interactive_data, cov_mat, prob, noise, treatment_periods, treatment_units)
    Y_true <- data$Y
    W_true <- data$W
    treated_units <- data$index
    
    if (length(Y_true) %% nrow(Y_true) != 0) {
  stop("Y_true dimensions are inconsistent with expected operations.")
}
if (length(W_true) %% nrow(W_true) != 0) {
  stop("W_true dimensions are inconsistent with expected operations.")
}
    
    #storing the estimates of the different methods
    estimate_did[expirement] <- did_estimate(Y_true, 
                                             nrow(Y_true) - treatment_units, 
                                             ncol(Y_true)-treatment_periods
                                            )
    
    
    estimate_mc[expirement] <- DWCP_TWFE_average(Y_true, W_true, treatment_units, lambda_unit = 0, lambda_time=0, lambda_nn=.6, treatment_periods)
    
    estimate_sdid[expirement] <- synthdid_estimate(Y_true, 
                                                   nrow(Y_true) - treatment_units, 
                                                   ncol(Y_true)- treatment_periods
                                                   )
    
    estimate_sc[expirement] <- sc_estimate(Y_true,
                                           nrow(Y_true) - treatment_units,
                                           ncol(Y_true)-treatment_periods
                                           )
    
    estimate_difp[expirement] <- DIFP_TWFE(Y_true, W_true, treatment_units, treatment_periods)
    estimate_dwcp[expirement] <- DWCP_TWFE_average(Y_true, W_true, treatment_units, lambda_unit = 1.5, lambda_time=.25, lambda_nn=.176)
  }

return(list(estimate_sdid, estimate_sc, estimate_did, estimate_mc,  estimate_difp,  estimate_dwcp))
  
}


```


```{r}
test_Y <- matrix(rnorm(100), 10, 10)
test_W <- matrix(rbinom(100, 1, 0.5), 10, 10)
print(did_estimate(test_Y, 5, 5))
print(DWCP_TWFE_average(test_Y, test_W, 5, lambda_unit = 0, lambda_time = 0, lambda_nn = 0.6, 5))

```


```{r}
# Test script for validating helper functions

# Generate mock inputs for testing
set.seed(42)
n <- 10  # Number of units
t <- 10  # Number of time periods

# Mock data
Y_true <- matrix(rnorm(n * t), n, t)
W_true <- matrix(rbinom(n * t, 1, 0.5), n, t)
treatment_units <- 3  # Number of treated units
treatment_periods <- 3  # Number of treated periods
lambda_unit <- 1.5
lambda_time <- 0.25
lambda_nn <- 0.6

# Mock "generate_data" output
mock_data <- list(
  Y = Y_true,
  W = W_true,
  index = sample(1:n, treatment_units)
)

# 1. Validate did_estimate
tryCatch({
  did_result <- did_estimate(Y_true, n - treatment_units, t - treatment_periods)
  print("did_estimate output:")
  print(did_result)
}, error = function(e) {
  print("Error in did_estimate:")
  print(e)
})

# 2. Validate DWCP_TWFE_average
tryCatch({
  dwcp_result <- DWCP_TWFE_average(Y_true, W_true, treatment_units, lambda_unit, lambda_time, lambda_nn, treatment_periods)
  print("DWCP_TWFE_average output:")
  print(dwcp_result)
}, error = function(e) {
  print("Error in DWCP_TWFE_average:")
  print(e)
})

# 3. Validate synthdid_estimate
tryCatch({
  synthdid_result <- synthdid_estimate(Y_true, n - treatment_units, t - treatment_periods)
  print("synthdid_estimate output:")
  print(synthdid_result)
}, error = function(e) {
  print("Error in synthdid_estimate:")
  print(e)
})

# 4. Validate sc_estimate
tryCatch({
  sc_result <- sc_estimate(Y_true, n - treatment_units, t - treatment_periods)
  print("sc_estimate output:")
  print(sc_result)
}, error = function(e) {
  print("Error in sc_estimate:")
  print(e)
})

# 5. Validate DIFP_TWFE
tryCatch({
  difp_result <- DIFP_TWFE(Y_true, W_true, treatment_units, treatment_periods)
  print("DIFP_TWFE output:")
  print(difp_result)
}, error = function(e) {
  print("Error in DIFP_TWFE:")
  print(e)
})
```






Calculating the RMSE of each method
```{r}
estimate_vector <- list(estimate_sdid, estimate_dwcp, estimate_mc, estimate_sc, estimate_difp, estimate_did)
name_vector <- c("estimate_sdid", "estimate_dwcp", "estimate_mc", "estimate_sc", "estimate_difp", "estimate_did")
error_vector <- rep(0,length(estimate_vector))

for(i in 1:length(error_vector)){
  error_vector[i] <- sqrt(mean(estimate_vector[[i]]^2))
}


```


Displaying the RMSE for each of the methods
```{r}
for(i in 1:length(error_vector)){
  cat("The RMSE of", name_vector[i], "is", error_vector[i], "\n")
}

cat("This is AR(2): ", ar_coef, "\n")
cat("This is scaled_sd: ", scaled_sd, "\n")
cat("Frobenius norm of F divided by the square root of N_total * T_total: ",norm(F_, type = "F") / sqrt(N_total * T_total), "\n")
cat("Frobenius norm of M, divided by the square root of N_total * T_total: ", norm(M, type = "F") / sqrt(N_total * T_total), "\n")

cat("This is trace of cov_mat divided by T_total: ", sqrt(sum(diag(cov_mat)) / T_total), "\n")
```


"list(estimate_sdid, estimate_sc, estimate_did, estimate_mc, estimate_difp,  estimate_dwcp)"

The following section is each of the Simulation Regimes:

Baseline:

```{r}
baseline <- table_generation(F_, M, cov_mat, pi, "norm", 10, 10, 100, 0)
```


"fixed_effects, interactive_data, cov_mat, prob, noise = "norm", treatment_periods, treatment_units, exp_num = 1000, ran_seed=0"

```{r}
for(x in baseline){
  print(mean(x))
}
```

[1] 0.0201319
[1] 0.01314062
[1] 0.04622447
[1] 0.001606847
[1] -0.0005750824


No Correlation:

```{r}
no_correlation <- table_generation(F_, M, diag(nrow(cov_mat)), pi, "norm", 10, 10, 100, 0)
```

No Interactive Effects:
```{r}
no_interactive <- table_generation(matrix(0,nrow(F_), ncol(F_)), M, cov_mat, pi, "norm", 10, 10, 100, 0)
```


No Fixed Effects:
```{r}
no_fixed <- table_generation(F_, matrix(0,nrow(M), ncol(M)), cov_mat,pi, "norm", 10, 10, 100, 0)
```


Only Noise:
```{r}
only_noise <- table_generation(F_, M, cov_mat, pi,"noise", 10, 10, 100, 0)
```


No Noise:
```{r}
no_noise <- table_generation(F_, M, cov_mat, pi, "none", 10, 10, 100, 0)
```


The following section is each of the simulation Regimes given a different assignment policy:

Gun Law:

```{r}
gun_law <- table_generation(F_, M, cov_mat, pi_g, "norm", 10, 10, 100, 0)
```

Abortion Law:

```{r}
abort_law <- table_generation(F_, M, cov_mat, pi_a, "norm", 10, 10, 100, 0)
```

Random:

The follow section is each of the simulation Regimes given a different Outcome Variable:

Hours:

U-Rate:

The following section is each of the simulation Regimes given a different Block Size:

Treatment_Post: 1

```{r}
periods_one <- table_generation(F_, M, cov_mat, pi_a, "norm", 1, 10, 100, 0)
```

Number of Units Treated: 1

```{r}
units_one <- table_generation(F_, M, cov_mat, pi_a, "norm", 10, 1, 100, 0)
```


Treatment_Post and Number of Units Treated: 1
```{r}
unit_period_one<- table_generation(F_, M, cov_mat, pi_a, "norm", 1, 1, 100, 0)

```




```{r}
# Function to compute bias and RMSE for a single vector
compute_metrics <- function(x, true_value = 0) {
  bias <- mean(x - true_value)
  rmse <- sqrt(mean((x - true_value)^2))
  return(c(bias = bias, rmse = rmse))
}

compute_and_save_metrics <- function(variables, true_value = 0, output_csv = "metrics_results.csv") {
  # Initialize an empty list to store results
  results_list <- list()
  
  # Loop through each variable (list of vectors)
  for (var_name in names(variables)) {
    variable <- variables[[var_name]]
    
    # Apply metrics computation to each vector in the variable
    metrics <- do.call(rbind, lapply(variable, compute_metrics, true_value = true_value))
    
    # Convert metrics to a data frame and add method/variable labels
    metrics_df <- as.data.frame(metrics)
    metrics_df$method <- seq_len(nrow(metrics_df))  # Method identifier
    metrics_df$variable <- var_name
    
    # Append to results list
    results_list[[var_name]] <- metrics_df
  }
  
  # Combine all results into a single data frame
  final_results <- do.call(rbind, results_list)
  
  # Reshape to wide format for CSV
  final_results_wide <- reshape(
    final_results,
    timevar = "variable",
    idvar = "method",
    direction = "wide"
  )
  
  # Save to CSV
  write.csv(final_results_wide, output_csv, row.names = FALSE)
  
  return(final_results_wide) # Return results for inspection
}


variables <- list(
  baseline = baseline,
  no_correlation = no_correlation,
  no_interactive = no_interactive,
  no_fixed = no_fixed,
  only_noise = only_noise,
  no_noise = no_noise,
  gun_law = gun_law,
  abort_law = abort_law,
  periods_one = periods_one,
  units_one = units_one,
  unit_period_one = unit_period_one
)

# Compute metrics and save results to CSV
final_results <- compute_and_save_metrics(variables, true_value = 0, output_csv = "metrics_results.csv")


print(final_results)

```



